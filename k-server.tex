Recently, the primal-dual technique has been used to explore new approaches to the $k$-server problem~\cite{bansal10:k-server}.
We first state the problem and describe the current state of the randomized $k$-server conjecture.
Then we briefly give an overview of the steps involved in applying the primal-dual approach to the $k$-server problem.

\subsection{The $k$-Server Problem}
The problem statement is as follows:
We have $k$ servers located at up to $k$ points in a metric space (essentially a set with a well-behaved distance function) and must serve requests appearing one after another.
Each request is a point in the metric space and we serve it by moving one of the $k$ servers to the request.
The goal is to minimize the total distance travelled by all servers.

The $k$-server problem is one of the most important problems in the field of online algorithms.
One reason is that many online problems can be reduced to the $k$-server problem.
A common example is the paging problem. For $n$ pages and cache size $k$, the problem can be modelled by a uniform metric space on $n$ points and $k$ servers.
In a uniform metric space, the distance between all points is equal to 1.

It is conjectured that there is a deterministic $k$-competitive algorithm for the $k$-server problem.
While the best algorithm for the general case is $2k -1$ competitive, there are $k$-competitive algorithms for some special metrics like trees.
For the randomized case, the conjectured competitive ratio is $O(\log k)$ and there has been recent work showing a $\tilde{O}(log^3 n \; log^2 k)$ approximation ratio~\cite{bansal11:randomized-k-server}.
While the results in \cite{bansal11:randomized-k-server} do not use the primal-dual approach for online algorithms, some of the techniques are inspired by problems that were first introduced in the context of the primal dual approach.

\subsection{Applying the Primal-Dual Approach to the $k$-Server Problem}
Due to the importance of the $k$-server conjecture, there is already a large body of work on tackling the problem.
In order to put the primal-dual approach to the $k$-server problem in context, we review the high-level ideas here.

One key difficulty of the $k$-server problem on general metric spaces is the weak structure of the distances between points.
The most promising approach for overcoming this obstacle is work on embedding general metrics into hierarchically separated trees (HST).
In an HST, the distance between a node $x$ and its parent $p(x)$ depends only on a constant $\alpha$ and the depth $i$: $d(x, p(x)) = \alpha^i$.
Moreover, all leaf nodes in an HST are at the same level.
When embedding a metric space into an HST, only the leaves of the HST correspond to points in the metric space.
General metrics can be embedded into distributions over HSTs with only logarithmic distortion of the distances.
This distortion would be still be sufficient to provide polylogarithmic competitive ratios for the randomized $k$-server problem.

Cote et al.\ use HSTs introduce an approach to the $k$-server problem based on recursively solving a simpler problem at each node of the tree.
Due to the structure of HSTs, we have a uniform metric space on the children of a given node (all children have the same distance to their common parent).
This approach seems promising because of the relatively simple structure of uniform metrics.
Moreover, there has been recent success using this technique to derive algorithms for a closely related problem (metrical task systems).
In the case of $k$-server, the problem we want to solve at each node of the tree is the \emph{Allocation problem}.

The Allocation problem is defined as follows:
We have a uniform metric space with $n$ points and up to $k$ servers.
For each request $t$, the number of available servers is limited to $k(t) \leq k$.
Each request is described by a cost vector $h^t = (h^t(0), \ldots, h^t(k))$ where $h^t(j)$ is the cost of serving the request $t$ with $j$ servers.
The cost vectors satisfy a natural property: the costs are monotonic, i.e.\ $h^t(j) \geq h^t(j + 1)$ (using more servers for a requests cannot increase the cost of serving it).
When a request comes in, we can move an arbitrary number of available servers to the request before serving it.
The total cost incurred by the algorithm is divided into two parts:
\begin{itemize}
\item \emph{Move-Cost}, the cost of moving servers.
\item \emph{Hit-Cost}, the cost of serving the requests.
\end{itemize}

Cote et al.\ showed that an algorithm for the Allocation problem satisfying the following conditions leads to a polylogarithmic algorithm for the randomized $k$-server problem.
Let OPT be the cost of the optimal algorithm for the Allocation problem.
Then the conditions are as follows:
\begin{itemize}
\item The total Hit-Cost is at most $(1 + \epsilon)$ OPT.
\item The total Move-Cost is at most polylog $\cdot$ OPT.
\end{itemize}
However, Cote et al.\ only showed how to solve the Allocation Problem with these bounds for a metric space consisting of two points.
The authors of~\cite{bansal10:k-server} used the primal-dual approach to find an algorithm for a restricted version of the Allocation Problem on uniform metrics.

\subsection{The Allocation-C Problem}
The Allocation-C problem introduces a second constraint on the cost vector $h^t$.
In addition to being monotonic, the cost vector is now also \emph{convex}.
This means that we have $h^t(j+1) - h^t(j +2) \leq h^t(j) - h^t(j + 1)$.
So the marginal improvement in Hit-Cost for adding a new server to the request decreases with the number of servers already at the request.
While convexity seems to be a natural property, it does not arise from the reduction of Cote et al.
However, the authors of~\cite{bansal10:k-server} claim that the convexity property might hold in a more aggregate sense.

The main result of~\cite{bansal10:k-server} is an online algorithm for Allocation-C with the following guarantees (OPT is the optimal offline cost):
\begin{itemize}
\item The service cost is at most $(1+\epsilon)$ OPT.
\item The movement cost is at most $O(\log\frac{k}{\epsilon})$ OPT.
\end{itemize}

Instead of using the primal dual approach from \cite{buchbinder09:survey} directly, the authors first reduce the Allocation-C problem to another online problem.

In the \emph{Caching with Costs} problem, we have a uniform metric on $l$ points and up to $k$ available servers.
For each request $t$, the number of available servers is limited to $k(t) \leq k$.
Each request is described by a cost vector $c^t = (c_1^t, \ldots, c^t_l)$.
We can redistribute servers among the $l$ points when a new request comes in.
In contrast to the Allocation-C problem, we now incur a cost $c_i^t$ for each point $i$ with no server present.
The total cost incurred by the algorithm is divided into the movement cost and the hit cost.

In order to reduce the Allocation-C problem with $n$ points to an instance of Caching with Costs, we introduce $k$ points in Caching with Costs for each original point in Allocation-C (so $l = nk$).
This allows us to model assigning up to $k$ servers to a point $i$ in Allocation-C by assigning servers to the $k$ points corresponding to $i$ in Caching with Costs (note that in Caching with Costs, it is only relevant whether a point is being served, not by how many servers).
Given a request $h^t$ to point $i$ in Allocation-C, we translate it into Caching with Costs accordingly: $c^t_{i,j} = (h^t_{i,j-1} - h_{i,j}^t)$.
This reduction shows that the hit and movement costs incurred in Caching with Costs are equal to those incurred in Allocation-C.
Conversely, we can also show how to reduce Caching with Costs to Allocation-C.
Hence a $c$-competitive algorithm for one problem implies a $c$-competitive algorithm for the other problem.

Before stating the linear program for Caching with Costs, we need one further lemma.
TODO: state request decomposition lemma
TODO: state LP

