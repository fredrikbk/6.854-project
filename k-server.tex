Recently, the primal-dual technique has been used to explore new approaches to the $k$-server problem~\cite{bansal10:k-server}.
Towards this end, the authors extend the primal-dual technique to deal with a wider range of LPs.
We first demonstrate the extended primal-dual technique with an example from~\cite{bansal10:k-server}.
Then we briefly give an overview of the steps involved in applying the primal-dual technique to the $k$-server problem.

\subsection{Extended Primal-Dual Technique}
One of the main limitations of the original primal-dual technique is the restriction to non-negative constraints and variables.
Using the basic approach described in section \ref{section:general_approach}, it is impossible to include constraints like $x_i \geq x_j$, which have to be converted to $x_i - x_j \geq 0$ in an LP (note the negative coefficient of $x_j$).
This condition restricts the set of problems we can model and makes some LP formulations more complex.
The main technical contribution of~\cite{bansal10:k-server} is to overcome these limitations.

We illustrate the extended primal-dual framework with the weighted caching problem.
In this problem, we have a cache of size $k$ and a total of $n$ pages, each with a weight $w_i$.
At each time step $t$, we receive a request for a page $p_t$.
If $p_t$ is not in the cache, we have to evict one page from the cache and replace it with $p_t$.
Fetching a page $p$ into the cache costs $w_p$.
The goal is to minimize the total cost of fetching pages.

We now present a randomized $O(\log k)$ competitive algorithm for the weighted caching problem.
While a randomized $O(\log k)$ competitive algorithm was known before~\cite{bansal10:k-server}, the following algorithm using the extended primal-dual technique has a significantly simpler analysis.
It is worth noting that the first randomized $O(\log k)$ competitive algorithm also uses the primal-dual technique~\cite{bansal07:weighted-paging}.

First, we define the primal LP.
\[
\textrm{($P$) : min}  \sum_{p=1}^n\sum_{t=1}^T w_p \cdot z_{p,t} + \sum_{t=1}^T \infty \cdot y_{p_t,t}
\]
\[
	\begin{array}{rr}
	\textrm{subject to :} & \\
		\forall t \textrm{ and } S \subseteq [n] \textrm{ with }|S| > k \textrm{ :} & \sum_{p\in S} y_{p,t} \geq |S| - k \\
		\forall t,p \textrm{ :} & z_{p,t} \geq y_{p,t-1} - y_{p,t} \\
		\forall t,p \textrm{ :} & z_{p,t}, y_{p,t} \geq 0 \\
	\end{array}
\]

The variable $y_{p,t}$ denotes the fraction of page $p$ missing at time $t$.
Since $z_{p,t} \geq y_{p,t-1} - y_{p,t}$, the variable $z_{p,t}$ denotes the fraction of page $p$ fetched at time $t$.
Hence the first term of the objective captures the total cost of fetching pages.
The second term of the objective guarantees that the requested page is always fetched into the cache.
The first constraint ensures that only a total of $k$ pages are in the cache at any time.

The dual LP is
\[
\textrm{($D$) : max}  \sum_{t=1}^T\sum_S (|S| - k) a_{S,t}
\]
\[
	\begin{array}{rr}
	\textrm{subject to :} & \\
	\forall t,p \neq p_t \textrm{ :} & \sum_{S: p \in S} a_{S,t} - b_{p, t+1} + b_{p,t} \leq 0 \\
	\forall t,p \textrm{ :} & b_{p,t} \leq w_p \\
	\forall t,p \textrm{ and } |S| > k \textrm{ :} & a_{t,S}, b_{p,t} \geq 0 \\
	\end{array}
\]

We now sketch the corresponding algorithm for iteratively constructing a solution to the primal.
The following relation between the primal and dual is maintained at all times:
\[
y_{p,t} \leftarrow \frac{1}{k} \cdot \left( \exp \left( \frac{b_{p,t+1}}{w_p} \cdot \ln(1+k) \right) - 1 \right)
\]
When a new request for page $p_t$ arrives, we set all variables $y_{p,t}$ for $p \neq p_t$ to the previous value for time $t-1$.
Moreover, we set $y_{p_t, t} = b_{p_t, t} = 0$ because page $p_t$ has to be in the cache.
This violates the first primal constraint.
In order to evict pages, we now increase $b_{p, t+1}$ for all pages that are (partially) in the cache.
Note that increasing $b_{p, t+1}$ decreases the corresponding $y_{p,t}$.
Moreover, we increase $a_{S,t}$ at the same rate so that the first dual constraint remains satisfied.
Here, $S$ is the set of pages that are at least partially in the cache.

By construction, the algorithm above always maintains feasible primal and dual solutions.
Moreover, we can show that the increase in the primal objective is at most $O(\log k)$ times the change in dual objective.
This is done by considering the derivatives of the primal and dual objectives with respect to $a_{S,t}$ and $b_{p,t+1}$.
Since the relative change in primal and dual is bounded by $O(\log k)$, this shows that the fractional solution is $O(\log k)$ competitive.
Moreover, the fractional solution can be rounded to a randomized algorithm with constant overhead in the competitive ratio~\cite{bansal07:weighted-paging}.


\subsection{The $k$-Server Problem}
The $k$-server problem is one of the most important problems in the field of online algorithms.
The problem statement is as follows:
We have $k$ servers located at up to $k$ points in a metric space (essentially a set with a well-behaved distance function) and must serve requests appearing one after another.
Each request is a point in the metric space and we serve it by moving one of the $k$ servers to the request.
The goal is to minimizhee the total distance travelled by all servers.

It is conjectured that there is a deterministic $k$-competitive algorithm for the $k$-server problem.
While the best algorithm for the general case is $2k -1$ competitive, there are $k$-competitive algorithms for some special metrics like trees.
For the randomized case, the conjectured competitive ratio is $O(\log k)$ and there has been recent work showing an $\tilde{O}(log^3 n \; log^2 k)$ approximation ratio~\cite{bansal11:randomized-k-server}.
While the results in \cite{bansal11:randomized-k-server} do not use the primal-dual approach for online algorithms, some of the techniques are inspired by problems that were first introduced in the context of the primal-dual approach.

One key difficulty of the $k$-server problem on general metric spaces is the weak structure of the distances between points.
The most promising approach for overcoming this obstacle is to embed general metrics into hierarchically separated trees (HSTs).
The distortion resulting from this embedding would still be sufficient to provide a polylogarithmic competitive ratio for the randomized $k$-server problem.
Moreover, HSTs allow the $k$-server problem to be split recursively into subproblems at each node.
The advantage of this approach is that it leads to simpler subproblems since all children of a node in an HST have the same distance (a \emph{uniform} metric space).

In~\cite{bansal10:k-server}, the authors solve a restricted version of this subproblem called the \emph{Allocation-C problem}.
It is defined as follows:
We have a uniform metric space with $n$ points and up to $k$ servers.
For each request $t$, the number of available servers is limited to $k(t) \leq k$.
Each request is described by a cost vector $h^t = (h^t(0), \ldots, h^t(k))$ where $h^t(j)$ is the cost of serving the request $t$ with $j$ servers.
The cost vectors satisfy two properties:
\begin{itemize}
\item The costs are monotonic, i.e.\ $h^t(j) \geq h^t(j + 1)$.
Using more servers for a request cannot increase the cost of serving it.
\item The cost vector is convex.
This means that we have $h^t(j+1) - h^t(j +2) \leq h^t(j) - h^t(j + 1)$.
So the marginal improvement achieved by adding a new server to the request decreases with the number of servers already at the request.
\end{itemize}
When a request comes in, we can move an arbitrary number of available servers to the request before serving it.
The total cost incurred by the algorithm is divided into two parts:
\begin{itemize}
\item \emph{Move-Cost}, the cost of moving servers.
\item \emph{Hit-Cost}, the cost of serving the requests.
\end{itemize}

The main contribution of~\cite{bansal10:k-server} to the randomized $k$-server conjecture is an online algorithm for Allocation-C with the following guarantees (OPT is the optimal offline cost):
\begin{itemize}
\item The service cost is at most $(1+\epsilon)$ OPT.
\item The movement cost is at most $O(\log\frac{k}{\epsilon})$ OPT.
\end{itemize}
This is a relevant result because Cote et al.\ showed that an algorithm satisfying similar conditions for the more general \emph{Allocation problem} would yield a polylogarithmic algorithm for the randomized $k$-server problem~\cite{cote08:k-server}.


\subsection{The Caching with Costs Problem}
Instead of using the primal-dual approach directly, the authors first reduce the Allocation-C problem to another online problem.
In the \emph{Caching with Costs} problem, we have a uniform metric on $l$ points and up to $k$ available servers.
For each request $t$, the number of available servers is limited to $k(t) \leq k$.
Each request is described by a cost vector $c^t = (c_1^t, \ldots, c^t_l)$.
We can redistribute servers among the $l$ points when a new request comes in.
In contrast to the Allocation-C problem, we now incur a cost $c_i^t$ for each point $i$ with no server present.
The total cost incurred by the algorithm is divided into the movement cost and the hit cost.

The authors show that Allocation-C and Caching with Costs can be reduced to each other in an online fashion.
Moreover, Caching with Costs can be simplified so that at time $t$, only one location $p_t$ has non-zero cost $c_{p_t}$.
The primal LP for Caching with Costs is:
\[
\textrm{($P$) : min}  \sum_{t=1}^T c_{p_t, t} \cdot y_{p_t,t} + \sum_{i=1}^n\sum_{t=1}^{T+1} z_{i,t}
\]
\[
	\begin{array}{rr}
	\textrm{subject to :} & \\
		\forall t \textrm{ and } S \subseteq [l] \textrm{ :} & \sum_{i : i \in S} y_{i,t} \geq |S| - k(t) \\
		\forall t,i \textrm{ :} & z_{i,t} \geq y_{i,t-1} - y_{i,t} \\
		\forall t,i \textrm{ :} & z_{i,t}, y_{i,t} \geq 0 \\
	\end{array}
\]


The LP is similar to the one for the weighted caching problem described earlier.
The variable $y_{i,t}$ indicates whether a server is located at point $i$ at time $t$.
Since $z_{i,t} \geq y_{i,t-1} - y_{i,t}$, the variable $z_{i,t}$ denotes the cost for moving a server to point $i$ at time $t$.
So the first term of the objective captures the hit cost and the second term the movement cost.
The first constraint guarantees that at most $k(t)$ servers are in use at time $t$.

The dual LP is:
\[
\textrm{($D$) : max}  \sum_{t=1}^T\sum_S (|S| - k(t)) \alpha_{t,S} + \sum_{i | \textrm{no server at }i} \gamma_i
\]
\[
	\begin{array}{rr}
	\textrm{subject to :} & \\
		\forall t,i \textrm{ :} & \sum_{S: i\in S} \alpha_{t,S} + \beta_{i,t} - \beta_{i,t+1} \leq c_{i,t}\\
		\forall t,i \textrm{ :} & \gamma_i \geq \beta_{i, 1} \\
		\forall t,i \textrm{ :} & 0 \leq \beta_{i, t} \leq 1\\
		\forall t \textrm { and } S \subseteq [l] \textrm{ :} & a_{t,S} \geq 0\\
	\end{array}
\]


The algorithm for Caching with Costs is similar to weighted paging.
Again, we maintain a relation between the primal and dual:
\[
y_{i,t} \leftarrow \frac{\epsilon}{1 + k} \left( \exp\left( \ln\left( 1 + \frac{1+k}{\epsilon} \right) \cdot \beta_{i, t+1} \right) - 1 \right)
\]
By considering the derivatives of the primal and dual objective functions, the authors prove the desired bounds for Caching with Costs which carry through to Allocation-C.
